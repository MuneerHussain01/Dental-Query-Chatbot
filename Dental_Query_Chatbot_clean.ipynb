{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o3Jds8_knP5h",
    "outputId": "10a74818-6920-4c33-ae58-73387406578a"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "4e41867af81343a48eb84cebba597233",
      "5059780c26d64845838f22f5c82d2a06",
      "8e7ef465e3a9415e8dc56dfeb0eab37f",
      "361c4f1366184506851deca0f8352222",
      "11d8c6d73c5741e9a0039af0ab96ff4d",
      "8282e1c4093c4fc5995a46a307b8786a",
      "b75951a4927f4d9e8f610b0616295349",
      "e9ef0a1b20cb485c8a7480b8789e7c85",
      "e53f7bb43fde43e1988d8f92d79321f9",
      "a838bf5e1efb4726acce9d360ce1aba9",
      "25022fe3491542af86a8479b2e15b4db",
      "0d04ee1a583e450ea1e7cce686ed81c4",
      "585716e3b7584dfc83324b037674a544",
      "56759255b5904e508effba37b074b456",
      "4cf14f4ca8524ad1b0c06497c4d75967",
      "e7dac46e14ba48c5847a3d1d36f0ed86",
      "ec605c7436b74b11a693af4309f084fe",
      "7b3c3765d8c24c9ebc0c46c1a297e7bb",
      "bfd0a6788f064fef8ac1fc4607f9ebd0",
      "de36f316b9114012bae6ce06af2eb933",
      "53fcf3051b6d499994da0b9f8baea0a9",
      "e3906229bd064fcf9f9de2f2eb17a5c0",
      "a8b5c0b0bac3489c9ab2ff39784bd481",
      "c036921197e54881bb1d6a00ea732365",
      "ee835c18477a42f78b35bd00af1d64ce",
      "7b6783aace224f1fb924334154b64b3a",
      "e7d2cfaa52db40d2ae32aa7b1da5b4af",
      "1971e8377cde4276bc11c6591538d51e",
      "c6be29c6635140f2b1ca64f70b605474",
      "c6bb6c4c5d2a4b798624954a79a87385",
      "26e52afa2c9148ca95a73969f6b0197d",
      "4f6d41425def4fe7948e8337fe0088d9",
      "9d1e53736afb42a29b6ce2069b6d207f",
      "07ab643909484d8aad54883d866c950e",
      "c609db55fee941468ed44395bd380062",
      "536a65dcd11245619aca2a2eb1afb542",
      "1207d2b31e804ce8b5661d366bfc84cd",
      "2f26cf679ca64499ae0cadffa26a5b72",
      "f3b2eb5bb34148acaecf8b0033d53b93",
      "80acfb9ad1a9443cae3a94a6f9b2c2dc",
      "cba683617de3434b853c0df43edcbe83",
      "2a0e5d96d2e74fc09d667afbe66c53b2",
      "b6afc4f2b35d46e185cd54c31f8d234b",
      "d2b6918944dd4ab7a394212401b580fb",
      "339ed5de6304403cb1e8998896a33405",
      "59ba6f455d1c4fe2a7f4aeb1fbf06481",
      "7e1967a927814c60891c39f013951780",
      "8a53e0eb528f4fa7b4b6ed279c8f73ac",
      "6b3a6ec490b24d429affb00a54fd79d5",
      "05efeb7750b448a58c7a103e826e2562",
      "e4b6302ea25646ec81f5e463f054c461",
      "20b391961ec64e9290cb426a3f7d5ec4",
      "9f962dcaeb504161a6cabf1f1dca7161",
      "545b2142583b4f5ca4b42011fd116c31",
      "430ec2994fde4852a6b68e89256995c0",
      "57a6b750319843b6b243203403b9b205",
      "2f7a8bd6288944d1a5526487acceaae7",
      "6a83719e446d4172be566846b5a7afb0",
      "de933b0e40e74856b30fcf6a44ba7bba",
      "a05216ea90514620a0df8275c5236b7d",
      "dc1952fc157c4770a54a258f17b2a8b9",
      "85fd5fba8bf24ea8895e51eaa34abd8b",
      "b24b083914f34c3da2c0a878cc43695c",
      "05e608d8482d49f680b574a300f37ded",
      "04a770f0f417499bb9df1935403f9e7f",
      "999a98169fbf411fa1759cf78e0a5991",
      "b8277ebe874e4a49a2537a63eca3d142",
      "95552d67e2844e99a7b6077bc59dd603",
      "9e57fec28ac64dd3ae8005894ed460fc",
      "59ff1b21b3fc49b6a5456fba0203bc3b",
      "3a80c64b857b4d08b85255887b967380",
      "9e91f22f4469428693f73743e8238afd",
      "a531b7fb35e64006834bb6914c50f002",
      "8718f441d10d4d1ca7c801c716e8fa75",
      "91379178d3c145e29577d30864efe3e8",
      "62f7d451b39a445487ac017534578d61",
      "49722ce015df4d6ea996b674b87a6247"
     ]
    },
    "id": "2ysDols2mCrc",
    "outputId": "e28acda8-4717-47f1-f278-4bdb63b7cd9a"
   },
   "outputs": [],
   "source": [
    "!pip install -U transformers datasets scikit-learn seaborn\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast,\n",
    "    DistilBertForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "# Disable Weights & Biases logging (optional)\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/dental_health_dataset.csv')\n",
    "label_encoder = LabelEncoder()\n",
    "df['label_encoded'] = label_encoder.fit_transform(df['label'])\n",
    "\n",
    "# Tokenizer and model\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased',\n",
    "    num_labels=len(label_encoder.classes_)\n",
    ")\n",
    "\n",
    "# Tokenization\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "# Prepare dataset\n",
    "dataset = Dataset.from_pandas(df[['text', 'label_encoded']])\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "tokenized_ds = dataset.map(tokenize, batched=True)\n",
    "tokenized_ds = tokenized_ds.rename_column(\"label_encoded\", \"labels\")\n",
    "tokenized_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    logging_dir='./logs'\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"]\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "# Predictions\n",
    "predictions = trainer.predict(tokenized_ds[\"test\"])\n",
    "y_true = predictions.label_ids\n",
    "y_pred = predictions.predictions.argmax(axis=1)\n",
    "\n",
    "# âœ… Fix: Use only labels/classes present in test set\n",
    "used_labels = unique_labels(y_true, y_pred)\n",
    "used_class_names = label_encoder.inverse_transform(used_labels)\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, labels=used_labels, target_names=used_class_names))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=used_labels)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=used_class_names, yticklabels=used_class_names, cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tDnJPhrMmHrU",
    "outputId": "3735e892-c660-42af-ba40-d020db1b7e8e"
   },
   "outputs": [],
   "source": [
    "pip install gradio joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "meNbFuFemKZa",
    "outputId": "957ca7d4-d33b-4f3a-e861-b6b6b1f8ebb8"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(label_encoder, \"label_encoder.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U2myTIDhmK87"
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"./results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 645
    },
    "id": "UkhMUPLumUp-",
    "outputId": "1e2efd9a-8029-43af-a419-6c0f075a1f11"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "\n",
    "# Load trained model and tokenizer\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"./results\")\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Load label encoder (you should save this during training)\n",
    "label_encoder = joblib.load(\"label_encoder.joblib\")\n",
    "\n",
    "def predict(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    pred = torch.argmax(probs, dim=1).item()\n",
    "    label = label_encoder.inverse_transform([pred])[0]\n",
    "    confidence = probs[0][pred].item()\n",
    "    return {label: float(round(confidence, 3))}\n",
    "\n",
    "interface = gr.Interface(\n",
    "    fn=predict,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Enter a dental query...\"),\n",
    "    outputs=gr.Label(num_top_classes=3),\n",
    "    title=\"Dental Query Chatbot\",\n",
    "    description=\"Enter a dental-related question and the model will classify it.\"\n",
    ")\n",
    "\n",
    "interface.launch()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
